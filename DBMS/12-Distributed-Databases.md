# Chapter 12: Distributed Databases

---

## ğŸ¯ What is a Distributed Database?

A **Distributed Database (DDB)** is a collection of logically interrelated databases distributed over a computer network.

### Key Characteristics
- Data stored at multiple sites
- Sites connected by a network
- Appears as single database to users

### ğŸ­ Analogy
> Distributed Database = **Bank with multiple branches**
> - Each branch has local data
> - All branches connected
> - Customer sees one "bank"

---

## ğŸ“Š Centralized vs Distributed

| Aspect | Centralized | Distributed |
|--------|-------------|-------------|
| Data location | Single site | Multiple sites |
| Failure | Single point | Partial availability |
| Scalability | Limited | High |
| Complexity | Low | High |
| Query cost | Memory/disk | + Network |

---

## ğŸ—ï¸ Distributed Database Architecture

### Types

```
                Distributed DB Architectures
                          â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                 â”‚                 â”‚
   Homogeneous      Heterogeneous      Federated
        â”‚                 â”‚                 â”‚
   Same DBMS        Different DBMS    Autonomous DBs
   Same Schema      Different Schema   Limited Integration
```

### Homogeneous DDBMS
- Same DBMS software at all sites
- Same data model and schema
- Easier to manage

### Heterogeneous DDBMS
- Different DBMS at different sites
- Translation/mapping needed
- More complex

---

## ğŸ“¦ Data Fragmentation

### Types of Fragmentation

#### 1. Horizontal Fragmentation
Split table **by rows** (based on selection condition).

```
Student Table:
â”Œâ”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”
â”‚ Roll â”‚  Name  â”‚ Dept â”‚
â”œâ”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¤
â”‚  1   â”‚ Alice  â”‚  CS  â”‚
â”‚  2   â”‚ Bob    â”‚  EE  â”‚
â”‚  3   â”‚ Carol  â”‚  CS  â”‚
â”‚  4   â”‚ Dave   â”‚  EE  â”‚
â””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”˜

Fragment 1 (CS site): Ïƒ_Dept='CS'(Student)
â”Œâ”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”
â”‚  1   â”‚ Alice  â”‚  CS  â”‚
â”‚  3   â”‚ Carol  â”‚  CS  â”‚
â””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”˜

Fragment 2 (EE site): Ïƒ_Dept='EE'(Student)
â”Œâ”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”
â”‚  2   â”‚ Bob    â”‚  EE  â”‚
â”‚  4   â”‚ Dave   â”‚  EE  â”‚
â””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”˜
```

#### 2. Vertical Fragmentation
Split table **by columns** (based on projection).

```
Employee Table:
â”Œâ”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ EmpIDâ”‚  Name  â”‚ Salary  â”‚  Dept  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1   â”‚ Alice  â”‚  50000  â”‚   CS   â”‚
â”‚  2   â”‚ Bob    â”‚  60000  â”‚   EE   â”‚
â””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Fragment 1 (HR site): Ï€_EmpID,Name,Dept(Employee)
â”Œâ”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ EmpIDâ”‚  Name  â”‚  Dept  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1   â”‚ Alice  â”‚   CS   â”‚
â”‚  2   â”‚ Bob    â”‚   EE   â”‚
â””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Fragment 2 (Finance site): Ï€_EmpID,Salary(Employee)
â”Œâ”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ EmpIDâ”‚ Salary  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1   â”‚  50000  â”‚
â”‚  2   â”‚  60000  â”‚
â””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Note: Include key (EmpID) in both for reconstruction!
```

#### 3. Mixed/Hybrid Fragmentation
Combination of horizontal and vertical.

```
First vertical, then horizontal on each fragment
OR
First horizontal, then vertical on each fragment
```

### Fragmentation Properties

| Property | Meaning |
|----------|---------|
| **Completeness** | All data preserved in fragments |
| **Reconstruction** | Can rebuild original from fragments |
| **Disjointness** | Fragments don't overlap (except keys) |

---

## ğŸ”„ Data Replication

### Types

| Type | Description | Pros | Cons |
|------|-------------|------|------|
| **No Replication** | Data at one site only | Simple updates | Single point of failure |
| **Full Replication** | Complete copy at each site | High availability | Update overhead |
| **Partial Replication** | Some data replicated | Balance of both | Complex management |

### Replication Strategies

#### Synchronous (Eager)
- Update all replicas in same transaction
- Strong consistency
- Higher latency

#### Asynchronous (Lazy)
- Update primary, propagate later
- Better performance
- Temporary inconsistency

---

## ğŸŒ Transparency in DDBMS

### Types of Transparency

| Transparency | User Unaware Of |
|--------------|-----------------|
| **Location** | Where data is stored |
| **Fragmentation** | How data is divided |
| **Replication** | How many copies exist |
| **Network** | Network details |
| **Transaction** | Distributed nature of transactions |

### Example
```sql
-- User writes simple query
SELECT * FROM Student WHERE Dept = 'CS';

-- DDBMS handles:
-- 1. Finding which site has CS students
-- 2. Routing query to correct fragment
-- 3. Combining results if needed
```

---

## ğŸ“Š Distributed Query Processing

### Steps
```
1. Query decomposition (parse, normalize)
2. Data localization (find fragments)
3. Global optimization (choose sites, join order)
4. Local optimization (each site optimizes locally)
```

### Cost Factors
```
Total Cost = Local I/O Cost + Network Cost

Network Cost = Data Transfer + Message Overhead

Network is often the bottleneck!
```

### Semijoin Optimization

Reduce data transfer for distributed joins.

```
Normal Join: Ship R to S's site, join there

Semijoin:
1. Project join column from R, send to S's site
2. At S, find matching tuples
3. Send matching S tuples back
4. Join at R's site

Useful when |Ï€_joinCol(R)| << |R|
```

---

## ğŸ” Distributed Transaction Management

### Issues
1. Atomicity across sites
2. Concurrency control across sites
3. Recovery across sites

### Distributed ACID

| Property | Challenge |
|----------|-----------|
| Atomicity | All sites commit or all abort |
| Consistency | Constraints across fragments |
| Isolation | Distributed locking/timestamps |
| Durability | Recovery at multiple sites |

---

## âœ… Two-Phase Commit (2PC)

### The Protocol

Ensures atomicity across distributed sites.

### Participants
- **Coordinator**: Transaction manager at originating site
- **Participants**: Transaction managers at other sites

### Phase 1: Voting Phase
```
Coordinator â†’ Participants: PREPARE?
Participants â†’ Coordinator: VOTE (Yes/No)

If participant votes Yes: Enters "prepared" state
                          Can commit or abort
If participant votes No:  Unilaterally aborts
```

### Phase 2: Decision Phase
```
If ALL votes = Yes:
    Coordinator â†’ Participants: COMMIT
    All participants commit

If ANY vote = No:
    Coordinator â†’ Participants: ABORT
    All participants abort
```

### State Diagram

```
                    COORDINATOR
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                                         â”‚
    â”‚    Initial â†’ Waiting â†’ Decided         â”‚
    â”‚                 â”‚         â”‚             â”‚
    â”‚            (collect    (commit/        â”‚
    â”‚             votes)      abort)         â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                    PARTICIPANT
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                                         â”‚
    â”‚    Initial â†’ Prepared â†’ Decided        â”‚
    â”‚                 â”‚          â”‚            â”‚
    â”‚           (voted yes)  (received       â”‚
    â”‚                        decision)       â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2PC Problems

| Problem | Description |
|---------|-------------|
| **Blocking** | Participants wait if coordinator fails |
| **Performance** | Extra round-trip for prepare/commit |
| **Uncertainty Period** | After voting yes, before decision |

---

## ğŸ“Š Three-Phase Commit (3PC)

### Improvement over 2PC
- Adds "pre-commit" phase
- Non-blocking in most failure scenarios

### Phases
```
Phase 1: Voting (same as 2PC)
Phase 2: Pre-commit (prepare to commit)
Phase 3: Commit (actual commit)
```

### Trade-off
- More messages
- Better failure handling
- Still not perfect (network partitions)

---

## ğŸ”’ Distributed Concurrency Control

### 1. Distributed Locking

#### Centralized Lock Manager
- One site manages all locks
- Simple but single point of failure

#### Primary Copy
- Each data item has primary site
- Lock requests go to primary

#### Distributed Lock Manager
- Lock at site where data resides
- More scalable

### 2. Distributed Timestamps

- Each site has local timestamp generator
- Combine with site ID for global uniqueness

```
Global Timestamp = (Local_Time, Site_ID)

(100, 1) < (100, 2) < (101, 1)
```

### 3. Distributed Optimistic Control

- Local validation at each site
- Global validation for distributed data

---

## ğŸ“± CAP Theorem

### Statement
A distributed system can provide at most **2 of 3** guarantees:

| Property | Meaning |
|----------|---------|
| **C - Consistency** | All nodes see same data at same time |
| **A - Availability** | Every request gets a response |
| **P - Partition Tolerance** | System works despite network failures |

### The Trade-off

```
         Consistency
            /\
           /  \
          /    \
         /      \
        /   CA   \    â† Possible only if no partitions
       /    â–²     \       (not practical in distributed)
      /     â”‚      \
     /      â”‚       \
    /â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€\
   /   CP   â”‚   AP    \
  â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼
Availability   Partition Tolerance

In distributed systems, P is non-negotiable
So choice is between C and A during partitions
```

### Examples

| System | Choice | Trade-off |
|--------|--------|-----------|
| Traditional RDBMS | CA | Not partition tolerant |
| MongoDB, Redis | CP | May be unavailable |
| Cassandra, DynamoDB | AP | Eventually consistent |

---

## ğŸ”„ Consistency Models

### Strong Consistency
- Read always returns latest write
- Expensive to implement

### Eventual Consistency
- Given enough time, all replicas converge
- Better availability

### Causal Consistency
- Causally related operations seen in order
- Non-causal operations can be concurrent

---

## ğŸ“ NoSQL Basics (Brief)

### Types of NoSQL Databases

| Type | Structure | Example | Use Case |
|------|-----------|---------|----------|
| **Key-Value** | Key â†’ Value | Redis, DynamoDB | Caching, sessions |
| **Document** | JSON-like docs | MongoDB, CouchDB | Content management |
| **Column-Family** | Column families | Cassandra, HBase | Analytics, time-series |
| **Graph** | Nodes + edges | Neo4j, Amazon Neptune | Social networks |

### BASE vs ACID

| ACID | BASE |
|------|------|
| Atomicity | **B**asically **A**vailable |
| Consistency | **S**oft state |
| Isolation | **E**ventual consistency |
| Durability | |

---

## âš ï¸ Common GATE Traps

### Trap 1: 2PC Guarantee
```
2PC guarantees atomicity, NOT deadlock-freedom or availability
Coordinator failure â†’ Blocking!
```

### Trap 2: CAP Theorem
```
Can't have all three in distributed system with partitions
Most systems choose AP or CP
CA only in single-site or always-connected systems
```

### Trap 3: Fragmentation Reconstruction
```
Horizontal: Union of fragments
Vertical: Natural join on key
Must preserve completeness!
```

### Trap 4: Semijoin Benefit
```
Semijoin reduces data transfer
NOT always better (overhead of extra messages)
Useful when projection is much smaller than table
```

---

## ğŸ§ª Practice Problems

### Q1: Fragmentation
```
Student(Roll, Name, Dept, CGPA)
Horizontal fragmentation by Dept (3 departments)
Vertical fragmentation: (Roll, Name) and (Roll, Dept, CGPA)

How to reconstruct?
1. Each horizontal fragment: Ïƒ_Dept='X'(Student)
2. Combine: F1 âˆª F2 âˆª F3
3. Vertical: F1 â‹ˆ F2 on Roll
```

### Q2: 2PC Scenario
```
Coordinator sends PREPARE to 3 participants
P1 votes Yes, P2 votes Yes, P3 votes No

Decision: ABORT (any No â†’ abort)
All participants abort (even P1, P2 who voted Yes)
```

### Q3: CAP Trade-off
```
During network partition:
- Choose Consistency: Some requests may fail
- Choose Availability: Some reads may return stale data

Can't have both during partition!
```

---

## ğŸ“Œ Chapter Summary

| Concept | Key Points |
|---------|------------|
| **Fragmentation** | Horizontal (rows), Vertical (columns), Mixed |
| **Replication** | Full, Partial, None |
| **Transparency** | Location, Fragmentation, Replication |
| **2PC** | Voting â†’ Decision, ensures atomicity |
| **CAP** | Choose 2 of 3 (Consistency, Availability, Partition) |
| **BASE** | Basically Available, Soft state, Eventually consistent |

---

## ğŸ“ Quick Revision Points

1. âœ… Horizontal = rows (selection), Vertical = columns (projection)
2. âœ… Vertical must include key for reconstruction
3. âœ… 2PC: All Yes â†’ Commit, Any No â†’ Abort
4. âœ… 2PC is blocking (coordinator failure)
5. âœ… CAP: Can only guarantee 2 of 3
6. âœ… Distributed needs P, so choose C or A
7. âœ… BASE trades consistency for availability

---

*Previous: [Recovery System](./11-Recovery-System.md) | Back to [Index](./README.md)*
